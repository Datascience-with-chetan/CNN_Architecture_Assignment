{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "- Filters (also called kernels) and feature maps are fundamental components of a Convolutional Neural Network (CNN) that work together to enable the network to learn and represent important features from input data, such as images.\n",
        "\n",
        "- Filters are small learnable matrices of weights that slide (convolve) over the input data. Each filter acts as a specialized feature detector, capturing local patterns like edges, corners, textures, or more complex shapes depending on the network depth. During training, CNNs learn the optimal filter weights to detect the most discriminative features for the task at hand, such as image classification or object detection. Filters help the network achieve translation invariance, meaning they can detect the same patterns regardless of their location in the input.\n",
        "\n",
        "- Feature maps are the outputs generated by convolving these filters over the spatial dimensions of the input. Each feature map corresponds to one filter and highlights the spatial locations where the filter’s pattern is detected. Multiple filters produce multiple feature maps, each emphasizing different aspects or patterns in the input. Feature maps can be seen as spatial activations showing the presence and intensity of specific features, which become progressively more abstract deeper in the network\n",
        "\n",
        "- Filters perform localized convolutions to extract specific features from the input.\n",
        "\n",
        "- Feature maps are the resulting spatial representations that encode how strongly and where those features appear.\n",
        "\n",
        "- Together, filters and feature maps enable CNNs to transform raw input data into meaningful hierarchical feature representations for further processing and prediction.\n",
        "\n",
        "- This interaction allows CNNs to recognize complex structures, reduce input dimensionality, and generalize effectively across positions and scales in visual data.\n"
      ],
      "metadata": {
        "id": "zRPcQq-7fnTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "- Padding and stride in CNNs can be simply explained as follows:\n",
        "\n",
        "- Padding is when you add extra pixels around the edge of an image (usually zeros) before applying a filter. This makes sure the size of the output feature map doesn’t get smaller after the convolution. For example, if you want the output size to be the same as the input size, you use padding. Without padding, the feature map shrinks because the filter can’t fully slide over the image edges.\n",
        "\n",
        "- Stride is the number of pixels the filter moves each time it slides over the image. A stride of 1 means the filter moves one pixel at a time, capturing detailed information and creating a larger output. A larger stride (like 2) means the filter jumps more pixels, which makes the output smaller because it scans fewer positions.\n",
        "\n",
        "- How padding and stride affect output size:\n",
        "\n",
        "  - More padding keeps the output size bigger or the same as the input.\n",
        "\n",
        "  - Larger stride makes the output smaller by skipping pixels.\n",
        "\n",
        "  - Padding keeps output size from shrinking.\n",
        "\n",
        "  - Stride controls how much the output size reduces by moving the filter faster over the input.\n",
        "\n",
        "  - This helps CNNs balance between detail and computation efficiency."
      ],
      "metadata": {
        "id": "pOCvoqK6gIep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "- In CNNs, the receptive field is the area of the input image that influences or \"sees\" the value of a particular feature in a specific layer. It tells us how much of the original input each neuron in the network is looking at.\n",
        "\n",
        "- The receptive field starts small in early layers, where neurons focus on small parts of the image (like edges or textures). As we go deeper into the network, the receptive field gets larger, allowing neurons to \"see\" bigger parts of the image and recognize more complex patterns, like objects or scenes.\n",
        "\n",
        "- The receptive field is important because:\n",
        "\n",
        "  - It ensures that deeper layers capture enough context of the input to make meaningful decisions.\n",
        "\n",
        "  - For tasks like object detection or segmentation, a large enough receptive field ensures the network considers the whole object, not just a small part.\n",
        "\n",
        "  - It guides how deep or complex a network needs to be to capture the necessary information.\n",
        "\n",
        "  - the receptive field defines the size of the input region that affects each output unit, and larger receptive fields in deep CNNs help the model understand global and detailed patterns at once."
      ],
      "metadata": {
        "id": "Ud07QXoCg4ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "- In simple and clear terms, here is how filter size and stride influence the number of parameters in a Convolutional Neural Network (CNN):\n",
        "\n",
        "- Filters and Parameters\n",
        "  - Filter Size: A filter (or kernel) is a small grid of numbers (weights) used to scan through the input image or feature map and detect features like edges or textures. The filter size is usually something like 3x3 or 5x5.\n",
        "\n",
        "  - Channels: Images have multiple channels (for example, three channels for RGB color images). The filter size applies to all these input channels. So, if the filter is 3x3 and the input has 3 channels, the filter weights cover all 3x3x3=27\n",
        "\n",
        "\n",
        "  - Number of Filters: A CNN layer has many such filters, like 32 or 64 filters. Each filter learns to look for a different feature.\n",
        "\n",
        "  - Calculating Parameters: Each weight in the filter is a parameter to learn. We also include one additional parameter per filter called bias. So, the formula for parameters in a convolutional layer is:\n",
        "\n",
        "- Stride and Parameters\n",
        "  - Stride means how many pixels the filter moves at a time while scanning the image.\n",
        "\n",
        "  - Stride affects the size (width and height) of the output feature map. A larger stride means the filter \"jumps\" further, resulting in a smaller output size.\n",
        "\n",
        "  - Importantly, stride does not affect the number of parameters, because the filter weights stay the same regardless of how the filter moves.\n",
        "\n",
        "- Bigger filters mean more weights to learn and more parameters.\n",
        "\n",
        "- More filters also mean more parameters.\n",
        "\n",
        "- Stride changes the output size but NOT the number of parameters.\n",
        "\n",
        "- This understanding helps design CNNs that balance model complexity (number of parameters) and computational efficiency depending on the task and data\n"
      ],
      "metadata": {
        "id": "zR5wNZkmhM-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare and contrast different CNN-based architectures like LeNet,AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "- Here is a descriptive comparison of CNN-based architectures LeNet, AlexNet, and VGG based on their depth, filter sizes, and performance:\n",
        "\n",
        "- LeNet\n",
        "  - Depth: LeNet is a relatively shallow network with about 7 layers in total (3 convolutional layers followed by 2 subsampling (pooling) layers and 2 fully connected layers).\n",
        "\n",
        "  - Filter Sizes: It uses relatively large filters in the first layers, such as 5x5 kernels.\n",
        "\n",
        "  - Performance: LeNet was one of the earliest CNNs and was designed for simple image recognition tasks like digit recognition (MNIST). It works well for small images but is less effective on complex datasets.\n",
        "\n",
        "  - Characteristics: Simpler and uses sigmoid or tanh activations originally; computationally inexpensive but not very deep.\n",
        "\n",
        "- AlexNet\n",
        "  - Depth: AlexNet is deeper than LeNet, with 8 layers (5 convolutional + 3 fully connected).\n",
        "\n",
        "  - Filter Sizes: The first convolutional layer uses large 11x11 filters with stride 4 for downsampling, then follow smaller filters like 5x5 and 3x3 in subsequent layers.\n",
        "\n",
        "  - Performance: AlexNet marked a breakthrough by winning the 2012 ImageNet challenge, significantly improving image classification accuracy on large, diverse datasets.\n",
        "\n",
        "  - Characteristics: Introduced ReLU activation for faster training, used overlapping max-pooling (3x3 kernel, stride 2), and leveraged GPUs for efficient training. It has about 60 million parameters.\n",
        "\n",
        "  - Impact: Started the era of modern deep CNNs with increased depth and computational power.\n",
        "\n",
        "- VGG\n",
        "  - Depth: VGG is much deeper, most commonly configured with 16 or 19 layers, using small 3x3 filters stacked sequentially.\n",
        "\n",
        "  - Filter Sizes: Uses only 3x3 convolutional filters consistently throughout the network but stacks many such layers to imitate large receptive fields.\n",
        "\n",
        "  - Performance: VGG achieved very high accuracy in image recognition (ImageNet), demonstrating that increasing depth with small filters can improve feature representation and overall performance.\n",
        "\n",
        "  - Characteristics: Uses uniform architecture with repeated blocks of convolutions followed by max-pooling, simple and elegant design, but very computationally expensive with around 138 million parameters.\n",
        "\n",
        "  - Impact: Popularized the idea that deeper networks with small filters are more effective, influencing many future architectures"
      ],
      "metadata": {
        "id": "mW67No04kG5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "# Import necessary modules\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0,1]\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "# Reshape images to add channel dimension (28, 28, 1)\n",
        "train_images = np.expand_dims(train_images, -1)\n",
        "test_images = np.expand_dims(test_images, -1)\n",
        "\n",
        "# One-hot encode labels\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Conv layer with 32 filters\n",
        "    MaxPooling2D((2, 2)),  # Downsample with max pooling\n",
        "    Conv2D(64, (3, 3), activation='relu'),  # More filters to learn complex features\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),  # Flatten feature maps into 1D vector\n",
        "    Dense(64, activation='relu'),  # Fully connected hidden layer\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 units for 10 classes\n",
        "])\n",
        "\n",
        "# Compile model with optimizer, loss, and metrics\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=5,\n",
        "          batch_size=64,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uWYITS8kr9J",
        "outputId": "ebffced0-6feb-461f-cb94-0589301e99e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - accuracy: 0.8541 - loss: 0.4713 - val_accuracy: 0.9732 - val_loss: 0.0843\n",
            "Epoch 2/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 49ms/step - accuracy: 0.9790 - loss: 0.0674 - val_accuracy: 0.9807 - val_loss: 0.0637\n",
            "Epoch 3/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 49ms/step - accuracy: 0.9862 - loss: 0.0435 - val_accuracy: 0.9871 - val_loss: 0.0453\n",
            "Epoch 4/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 48ms/step - accuracy: 0.9905 - loss: 0.0314 - val_accuracy: 0.9879 - val_loss: 0.0433\n",
            "Epoch 5/5\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 48ms/step - accuracy: 0.9926 - loss: 0.0235 - val_accuracy: 0.9858 - val_loss: 0.0467\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9880 - loss: 0.0377\n",
            "Test accuracy: 0.9904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode target labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "pK3KU8rofmiD",
        "outputId": "80421d84-ebcb-46a8-9665-5f7292f130b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m262,272\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,810\u001b[0m (1.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,810</span> (1.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,810\u001b[0m (1.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,810</span> (1.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyz1otStfUVd",
        "outputId": "0f678733-d155-4709-d79d-cbf034d5bc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301925\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.189403\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.129469\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.211373\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.561667\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.161684\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.173721\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.184854\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.043853\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.226780\n",
            "\n",
            "Test set: Average loss: 0.0441, Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.108281\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.035120\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.029112\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.077925\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.240545\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.165399\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.103926\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.123762\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.085180\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.022188\n",
            "\n",
            "Test set: Average loss: 0.0405, Accuracy: 9881/10000 (98.81%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.016613\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.046789\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.010483\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.132065\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.050353\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.086073\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.024164\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.012274\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.140915\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.038115\n",
            "\n",
            "Test set: Average loss: 0.0340, Accuracy: 9880/10000 (98.80%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.045084\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.071655\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.005222\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.084722\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.123780\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.043325\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.179920\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.069934\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.011529\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.050677\n",
            "\n",
            "Test set: Average loss: 0.0322, Accuracy: 9897/10000 (98.97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.038335\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.091749\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.044934\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.006118\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.112872\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.031141\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.112654\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.008146\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.031967\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.009645\n",
            "\n",
            "Test set: Average loss: 0.0289, Accuracy: 9913/10000 (99.13%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#8.Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std for MNIST\n",
        "])\n",
        "\n",
        "# Load MNIST training and test datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # input channels=1 (grayscale), output=32 filters\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(9216, 128)  # 9216 = 64 * 12 * 12 (computed after conv and pooling)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes for digits 0-9\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))      # Conv1 + ReLU\n",
        "        x = F.relu(self.conv2(x))      # Conv2 + ReLU\n",
        "        x = self.pool(x)               # Max pooling\n",
        "        x = self.dropout1(x)           # Dropout\n",
        "        x = torch.flatten(x, 1)        # Flatten from (batch, features, h, w) to (batch, features)\n",
        "        x = F.relu(self.fc1(x))        # Fully connected + ReLU\n",
        "        x = self.dropout2(x)           # Dropout\n",
        "        x = self.fc2(x)                # Output layer\n",
        "        return F.log_softmax(x, dim=1) # Log probabilities for NLLLoss\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance and move to device\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Training loop\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()          # Zero the gradients\n",
        "        output = model(data)           # Forward pass\n",
        "        loss = criterion(output, target)  # Calculate loss\n",
        "        loss.backward()                # Backpropagation\n",
        "        optimizer.step()               # Update weights\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Evaluation function\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() * data.size(0)  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({accuracy:.2f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "# Run training and testing\n",
        "num_epochs = 5\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Suppose you have image data as numpy arrays\n",
        "# images: a numpy array of shape (num_samples, height, width, channels)\n",
        "# labels: corresponding labels, integer encoded (num_samples,)\n",
        "\n",
        "# Example placeholder arrays (replace with your actual data loaded in numpy arrays)\n",
        "images = np.random.random((1000, 150, 150, 3))  # 1000 RGB images of 150x150\n",
        "labels = np.random.randint(0, 2, 1000)          # Binary labels\n",
        "\n",
        "# Normalize pixel values\n",
        "images = images.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels assuming two classes\n",
        "labels = to_categorical(labels, num_classes=2)\n",
        "\n",
        "# Create an ImageDataGenerator with augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Flow from numpy arrays (batches of images and labels)\n",
        "train_generator = datagen.flow(images, labels, batch_size=32)\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the generator\n",
        "model.fit(train_generator, steps_per_epoch=len(images)//32, epochs=10)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8_pVFO2myaC",
        "outputId": "9048ed92-89f2-46db-8639-807cf2da4958"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 709ms/step - accuracy: 0.5046 - loss: 0.6938\n",
            "Epoch 2/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 494us/step - accuracy: 0.5938 - loss: 0.6922 \n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 736ms/step - accuracy: 0.5161 - loss: 0.6930\n",
            "Epoch 4/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - accuracy: 0.5000 - loss: 0.6932 \n",
            "Epoch 5/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 717ms/step - accuracy: 0.5019 - loss: 0.6932\n",
            "Epoch 6/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - accuracy: 0.5938 - loss: 0.6917 \n",
            "Epoch 7/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 720ms/step - accuracy: 0.4899 - loss: 0.6934\n",
            "Epoch 8/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - accuracy: 0.5938 - loss: 0.6921 \n",
            "Epoch 9/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 714ms/step - accuracy: 0.5183 - loss: 0.6930\n",
            "Epoch 10/10\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - accuracy: 0.5625 - loss: 0.6921 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7edb39037290>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "- To build and deploy a CNN model that classifies chest X-ray images into \"Normal\" and \"Pneumonia\" categories for a medical imaging web app, here is an end-to-end approach:\n",
        "\n",
        "- Data Preparation\n",
        "  - Dataset: Use a publicly available chest X-ray dataset (e.g., the Pneumonia dataset by Kermany et al.) or proprietary clinical data.\n",
        "\n",
        "  - Data Organization: Organize images into two folders, Normal and Pneumonia, for training, validation, and test sets.\n",
        "\n",
        "  - Preprocessing: Resize images to a fixed input size (e.g., 150x150 pixels). Normalize pixel values to. Use data augmentation like random rotation, shifting, and flipping to increase dataset variability and reduce overfitting.\n",
        "\n",
        "  - Data Loading: Use Keras ImageDataGenerator to load, batch, and augment images from directories.\n",
        "\n",
        "- Model Training\n",
        "  - Model Architecture: Build a CNN with multiple convolutional layers followed by max pooling, ReLU activations, dropout for regularization, and fully connected layers ending with a sigmoid or softmax layer for binary classification.\n",
        "\n",
        "  - Transfer Learning (optional): Use pre-trained models like VGG16, ResNet, or DenseNet as feature extractors and fine-tune on the pneumonia dataset to boost performance with limited data.\n",
        "\n",
        "  - Compile: Use Adam optimizer, binary cross-entropy loss for two classes, and metrics like accuracy and AUC.\n",
        "\n",
        "  - Training: Train the model with mini-batches, validate on a separate set, monitor for overfitting, and save the best model weights.\n",
        "\n",
        "- Model Evaluation and Testing\n",
        "  - Evaluate on a held-out test set measuring accuracy, precision, recall, F1-score, and AUC.\n",
        "\n",
        "  - Visualize confusion matrices and ROC curves to understand model strengths and weaknesses.\n",
        "\n",
        "  - Perform error analysis to assess misclassified cases and consider lung segmentation or image enhancement preprocessing if needed.\n",
        "\n",
        "- Model Deployment Using Streamlit\n",
        "  - Model Export: Save the trained model using Keras .h5 or TensorFlow SavedModel format.\n",
        "\n",
        "  - Streamlit App:\n",
        "\n",
        "    - Build a UI that allows users to upload chest X-ray images.\n",
        "\n",
        "    - Preprocess uploaded images to the required input size and normalization.\n",
        "\n",
        "    - Load the trained CNN model.\n",
        "\n",
        "    - Run inference on uploaded images to classify them as \"Normal\" or \"Pneumonia.\"\n",
        "\n",
        "    - Display the prediction and optionally confidence score.\n",
        "\n",
        "  - Deployment:\n",
        "\n",
        "    - Deploy the Streamlit app on a cloud platform (e.g., Streamlit Cloud, AWS, Azure).\n",
        "\n",
        "    - Ensure the app supports GPU acceleration if needed and has necessary integrations for patient data privacy.\n",
        "\n",
        "Example Components in Code\n",
        "  - Use ImageDataGenerator for data augmentation and loading.\n",
        "\n",
        "  - Define CNN architecture or load pre-trained models from Keras.\n",
        "\n",
        "  - Use model.fit() to train, model.evaluate() to test.\n",
        "\n",
        "  - Streamlit app structure to handle image upload, preprocessing, and prediction display."
      ],
      "metadata": {
        "id": "VaGVIttZnIeV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tiwHSqTPnBv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}